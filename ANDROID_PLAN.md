# Android Implementation for react-native-vision-ml

## Status: âœ… Core Implementation Complete

All major components have been implemented. Ready for integration testing.

## Implementation Summary

### âœ… Completed Components

| Component | File | Status |
|-----------|------|--------|
| NMS (Non-Maximum Suppression) | `NMS.kt` | âœ… Complete |
| YOLO Parser | `YOLOParser.kt` | âœ… Complete |
| Image Decoder | `ImageDecoder.kt` | âœ… Complete |
| ONNX Inference | `ONNXInference.kt` | âœ… Complete |
| Video Analyzer | `VideoAnalyzer.kt` | âœ… Complete |
| ML Kit Analyzer | `MLKitAnalyzer.kt` | âœ… Complete |
| RN Bridge Module | `VisionMLModule.kt` | âœ… Complete |
| Package Registration | `VisionMLPackage.kt` | âœ… Complete |
| Build Configuration | `build.gradle` | âœ… Complete |
| Manifest | `AndroidManifest.xml` | âœ… Complete |

### Feature Mapping: iOS â†’ Android

| iOS Feature | Android Implementation | Status |
|-------------|----------------------|--------|
| ONNX Runtime (Swift/ObjC) | ONNX Runtime Android (Kotlin) | âœ… |
| Vision Framework | Google ML Kit | âœ… |
| AVFoundation (video) | MediaMetadataRetriever | âœ… |
| PHAsset | MediaStore/ContentResolver | âœ… |
| Live Activity | Foreground Service (stub) | ğŸ”¶ Stub |
| CoreML acceleration | NNAPI delegate | âœ… |
| SensitiveContentAnalysis | N/A (iOS 17+ only) | âŒ iOS-only |

## Architecture

```
VisionMLModule.kt (React Native Bridge)
    â”œâ”€â”€ ONNXInference.kt
    â”‚   â”œâ”€â”€ ImageDecoder.kt (letterbox resize, EXIF handling)
    â”‚   â”œâ”€â”€ YOLOParser.kt (output parsing)
    â”‚   â””â”€â”€ NMS.kt (non-maximum suppression)
    â”‚
    â”œâ”€â”€ VideoAnalyzer.kt
    â”‚   â”œâ”€â”€ ScanMode: quick_check, sampled, thorough, binary_search, full_short_circuit
    â”‚   â””â”€â”€ MediaMetadataRetriever for frame extraction
    â”‚
    â””â”€â”€ MLKitAnalyzer.kt
        â”œâ”€â”€ Face Detection
        â”œâ”€â”€ Pose Detection
        â”œâ”€â”€ Image Labeling (animal detection)
        â””â”€â”€ Text Recognition
```

## API Parity with iOS

All iOS methods are implemented:

```kotlin
// Detector Management
createDetector(modelPath, classLabels, inputSize) â†’ detectorId
detect(detectorId, imageUri, confThreshold, iouThreshold) â†’ detections
disposeDetector(detectorId)
disposeAllDetectors()

// Video Analysis
analyzeVideo(detectorId, assetId, mode, sampleInterval, confThreshold)
quickCheckVideo(detectorId, assetId, confThreshold)

// ML Kit (Vision Framework equivalent)
analyzeAnimals(assetId)
analyzeHumanPose(assetId)
analyzeComprehensive(assetId)

// Progress Notification (Live Activity equivalent)
isLiveActivityAvailable() â†’ true (always available on Android)
startVideoScanActivity(videoName, duration, mode)
updateVideoScanActivity(progress, phase, nsfwCount, framesAnalyzed)
endVideoScanActivity(nsfwCount, framesAnalyzed, isNSFW)
```

## Dependencies

```gradle
// ONNX Runtime
implementation 'com.microsoft.onnxruntime:onnxruntime-android:1.16.0'

// ML Kit
implementation 'com.google.mlkit:pose-detection:18.0.0-beta4'
implementation 'com.google.mlkit:face-detection:16.1.6'
implementation 'com.google.mlkit:image-labeling:17.0.8'
implementation 'com.google.mlkit:text-recognition:16.0.0'

// Kotlin Coroutines
implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-core:1.7.3'
implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-android:1.7.3'
```

## Key Differences from iOS

1. **Y-axis orientation**: Android Bitmap has origin at top-left (same as standard image coordinates), so no Y-flip needed in YOLOParser (unlike iOS CGContext which has origin at bottom-left)

2. **EXIF handling**: Android requires explicit ExifInterface for rotation, handled in ImageDecoder

3. **Video frame extraction**: Uses `MediaMetadataRetriever.getFrameAtTime()` instead of `AVAssetImageGenerator`

4. **ML Kit vs Vision**: Similar APIs but different class names and result formats

5. **Live Activity**: iOS-specific feature - Android uses foreground service with notification (stubbed)

## Testing

```bash
# Build the Android library
cd android
./gradlew assembleDebug

# In the consuming app
npx react-native run-android
```

## Remaining Work

1. **Foreground Service**: Implement actual progress notification for video scanning
2. **Integration Testing**: Test with actual ONNX models on Android device
3. **Performance Tuning**: Profile NNAPI acceleration on various devices
4. **Error Handling**: Add more detailed error messages for debugging

## Notes

- ONNX models are cross-platform - same .onnx file works on both iOS and Android
- NNAPI acceleration is attempted automatically, falls back to CPU if unavailable
- ML Kit models are downloaded on first use (requires network)
- Video frame extraction is simpler on Android than iOS
